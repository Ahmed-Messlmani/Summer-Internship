{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM,Flatten\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus = pd.read_csv(r\"D:\\\\NLP project\\\\dataset\\\\final\\\\newcleandata2.csv\",encoding='latin-1')\n",
    "Corpus['title'].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Corpus['title'],Corpus['label'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1934957\n",
      "1934957\n",
      "483740\n",
      "483740\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(Train_X))\n",
    "print(len(Train_Y))\n",
    "print(len(Test_X))\n",
    "print(len(Test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = LabelEncoder()\n",
    "Train_Y = Encoder.fit_transform(Train_Y.astype(str))\n",
    "Test_Y = Encoder.fit_transform(Test_Y.astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels_final = np.array(Train_Y)\n",
    "testing_labels_final = np.array(Test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 700000\n",
    "max_length = 200\n",
    "trunc_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(Train_X)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(Train_X)\n",
    "padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(Test_X)\n",
    "testing_padded = pad_sequences(testing_sequences,maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings_index = {};\n",
    "with open('D:\\\\NLP project\\\\dataset\\\\final\\\\glove.6B.100d.txt',encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split();\n",
    "        word = values[0];\n",
    "        coefs = np.asarray(values[1:], dtype='float32');\n",
    "        embeddings_index[word] = coefs;\n",
    "\n",
    "embeddings_matrix = np.zeros((vocab_size+1,100));\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word);\n",
    "    if embedding_vector is not None:\n",
    "        embeddings_matrix[i] = embedding_vector;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 200, 100)          70000100  \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 200, 128)          84480     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 25600)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 25601     \n",
      "=================================================================\n",
      "Total params: 70,110,181\n",
      "Trainable params: 110,081\n",
      "Non-trainable params: 70,000,100\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size+1, 100, input_length=max_length, weights=[embeddings_matrix], trainable=False))\n",
    "model.add(Bidirectional(LSTM(64,return_sequences=True)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1934957 samples, validate on 483740 samples\n",
      "Epoch 1/2\n",
      "1934957/1934957 [==============================] - 8944s 5ms/step - loss: 0.0324 - acc: 0.9893 - val_loss: 0.0246 - val_acc: 0.9919\n",
      "Epoch 2/2\n",
      "1934957/1934957 [==============================] - 9619s 5ms/step - loss: 0.0226 - acc: 0.9927 - val_loss: 0.0230 - val_acc: 0.9927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24143943888>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs =2\n",
    "model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving\n",
    "import pickle\n",
    "with open('D:\\\\NLP project\\\\LSTMFinal model-Copy1tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('D:\\\\NLP project\\\\models\\\\LSTMFinal model-Copy1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01262252]]\n",
      "appropriate context\n"
     ]
    }
   ],
   "source": [
    "padding_type='post'\n",
    "sentence = [\"i'm going to reach my goal\"]\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "x=model.predict(padded)\n",
    "print(x)\n",
    "if x>0.5:\n",
    "    print(\"inapprpriate context\")\n",
    "else:\n",
    "    print(\"appropriate context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9999994]]\n",
      "apprpriate context\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "max_length = 120\n",
    "trunc_type='post'\n",
    "f = open('D:/NLP project/models/tokenizer.pickle', 'rb')\n",
    "tokenizer = pickle.load(f)\n",
    "model =tf.keras.models.load_model('D:/NLP project/models/LSTMFinal Model')\n",
    "padding_type='post'\n",
    "sentence = [\"when i go to the school i see a bitch which get fucked by an old man \"]\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "x=model.predict(padded)\n",
    "print(x)\n",
    "if x>0.5:\n",
    "    print(\"apprpriate context\")\n",
    "else:\n",
    "    print(\"inappropriate context\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
